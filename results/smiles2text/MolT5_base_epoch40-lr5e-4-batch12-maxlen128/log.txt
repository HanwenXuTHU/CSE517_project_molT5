2023-03-08 08:48:18,190 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 08:48:18,191 - __main__ - INFO - Epoch: 0, Loss: 1.9455652236938477, Best Loss: 1.406640454910804
2023-03-08 09:07:01,679 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 09:07:01,680 - __main__ - INFO - Epoch: 1, Loss: 1.5980300903320312, Best Loss: 0.946422928701277
2023-03-08 09:25:15,115 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 09:25:15,115 - __main__ - INFO - Epoch: 2, Loss: 1.2108479738235474, Best Loss: 0.7663544539524162
2023-03-08 09:42:52,388 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 09:42:52,389 - __main__ - INFO - Epoch: 3, Loss: 1.2083650827407837, Best Loss: 0.7300839429532274
2023-03-08 09:59:59,776 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 09:59:59,777 - __main__ - INFO - Epoch: 4, Loss: 0.9945483803749084, Best Loss: 0.6363991679272789
2023-03-08 10:16:55,047 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 10:16:55,058 - __main__ - INFO - Epoch: 5, Loss: 0.9109935164451599, Best Loss: 0.5849561662129731
2023-03-08 10:33:14,012 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 10:33:14,015 - __main__ - INFO - Epoch: 6, Loss: 0.883122980594635, Best Loss: 0.5458315317181575
2023-03-08 10:49:04,686 - __main__ - INFO - Epoch: 7, Loss: 1.0436291694641113, Best Loss: 0.5458315317181575
2023-03-08 11:04:40,536 - __main__ - INFO - Epoch: 8, Loss: 0.9789435863494873, Best Loss: 0.5458315317181575
2023-03-08 11:20:09,117 - __main__ - INFO - Epoch: 9, Loss: 0.9988846778869629, Best Loss: 0.5458315317181575
2023-03-08 11:35:51,529 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 11:35:51,530 - __main__ - INFO - Epoch: 10, Loss: 0.871780514717102, Best Loss: 0.515066519703554
2023-03-08 11:51:32,002 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 11:51:32,006 - __main__ - INFO - Epoch: 11, Loss: 0.8727826476097107, Best Loss: 0.49340750417415674
2023-03-08 12:06:59,629 - __main__ - INFO - Epoch: 12, Loss: 0.9575261473655701, Best Loss: 0.49340750417415674
2023-03-08 12:22:51,050 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 12:22:51,058 - __main__ - INFO - Epoch: 13, Loss: 0.8214403986930847, Best Loss: 0.48724969579041827
2023-03-08 12:38:18,766 - __main__ - INFO - Epoch: 14, Loss: 0.8590106964111328, Best Loss: 0.48724969579041827
2023-03-08 12:54:03,993 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 12:54:03,998 - __main__ - INFO - Epoch: 15, Loss: 0.8900008797645569, Best Loss: 0.4863974339720134
2023-03-08 13:09:48,503 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 13:09:48,512 - __main__ - INFO - Epoch: 16, Loss: 0.8047469258308411, Best Loss: 0.4655184100071589
2023-03-08 13:25:33,264 - __main__ - INFO - save model to ./results/smiles2text/MolT5_base_epoch40-lr5e-4-batch12-maxlen128/model_best.pt
2023-03-08 13:25:33,271 - __main__ - INFO - Epoch: 17, Loss: 0.8150758147239685, Best Loss: 0.4552952070059119
